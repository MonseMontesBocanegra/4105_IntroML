{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOU5+2ql2PCSWM23M0q09aP",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MonseMontesBocanegra/4105_IntroML/blob/Assignments/Monse_MontesBocanegra_801297280_Homework1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Part 1\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "url = 'https://raw.githubusercontent.com/HamedTabkhi/Intro-to-ML/main/Dataset/D3.csv'\n",
        "\n",
        "df = pd.read_csv(url)\n",
        "print(df.head())\n",
        "\n",
        "X1 = df['X1'].values\n",
        "X2 = df['X2'].values\n",
        "X3 = df['X3'].values\n",
        "Y = df['Y'].values\n",
        "\n",
        "# Gradient Descent Function\n",
        "def gradient_descent(X, Y, learning_rate, epochs):\n",
        "    m = 0  # slope (theta)\n",
        "    b = 0  # intercept\n",
        "    n = len(Y)\n",
        "    losses = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        Y_pred = m * X + b\n",
        "        loss = (1 / (2 * n)) * np.sum((Y_pred - Y) ** 2)\n",
        "        losses.append(loss)\n",
        "\n",
        "        # Gradient calculation\n",
        "        dm = -(1/n) * np.sum(X * (Y - Y_pred))  # derivative of loss with respect to m\n",
        "        db = -(1/n) * np.sum(Y - Y_pred)        # derivative of loss with respect to b\n",
        "\n",
        "        # Update parameters\n",
        "        m = m - learning_rate * dm\n",
        "        b = b - learning_rate * db\n",
        "\n",
        "    return m, b, losses\n",
        "\n",
        "# Function to plot the results\n",
        "def plot_results(X, Y, m, b, losses, title):\n",
        "    # Plot regression line\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    # Subplot 1: Linear Regression Line\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.scatter(X, Y, color='blue')\n",
        "    plt.plot(X, m * X + b, color='red')\n",
        "    plt.title(f\"Linear Regression with {title}\")\n",
        "    plt.xlabel('X')\n",
        "    plt.ylabel('Y')\n",
        "\n",
        "    # Subplot 2: Loss over Iterations\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(losses)\n",
        "    plt.title(f\"Loss over Iterations with {title}\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "# Training with different learning rates\n",
        "learning_rates = [0.1, 0.05, 0.01]  # Exploring different learning rates\n",
        "epochs = 1000  # Number of iterations\n",
        "\n",
        "for lr in learning_rates:\n",
        "    print(f\"Training with learning rate: {lr}\")\n",
        "\n",
        "    # X1\n",
        "    m1, b1, losses1 = gradient_descent(X1, Y, lr, epochs)\n",
        "    plot_results(X1, Y, m1, b1, losses1, f'X1 (LR={lr})')\n",
        "\n",
        "    # X2\n",
        "    m2, b2, losses2 = gradient_descent(X2, Y, lr, epochs)\n",
        "    plot_results(X2, Y, m2, b2, losses2, f'X2 (LR={lr})')\n",
        "\n",
        "    # X3\n",
        "    m3, b3, losses3 = gradient_descent(X3, Y, lr, epochs)\n",
        "    plot_results(X3, Y, m3, b3, losses3, f'X3 (LR={lr})')\n",
        "\n",
        "    # Report final models and losses for each variable at this learning rate\n",
        "    print(f\"Model for X1 (LR={lr}): Y = {m1:.4f}*X1 + {b1:.4f}\")\n",
        "    print(f\"Final loss for X1: {losses1[-1]:.4f}\")\n",
        "\n",
        "    print(f\"Model for X2 (LR={lr}): Y = {m2:.4f}*X2 + {b2:.4f}\")\n",
        "    print(f\"Final loss for X2: {losses2[-1]:.4f}\")\n",
        "\n",
        "    print(f\"Model for X3 (LR={lr}): Y = {m3:.4f}*X3 + {b3:.4f}\")\n",
        "    print(f\"Final loss for X3: {losses3[-1]:.4f}\")\n",
        "\n",
        "    # Find the variable with the lowest loss at this learning rate\n",
        "    lowest_loss_var = min([('X1', losses1[-1]), ('X2', losses2[-1]), ('X3', losses3[-1])], key=lambda x: x[1])\n",
        "    print(f\"The variable with the lowest loss (LR={lr}): {lowest_loss_var[0]} with a final loss of {lowest_loss_var[1]:.4f}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "z01MDw5dKpxK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Part 2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# URL of the dataset\n",
        "url = 'https://raw.githubusercontent.com/HamedTabkhi/Intro-to-ML/main/Dataset/D3.csv'\n",
        "\n",
        "# Reading the dataset\n",
        "df = pd.read_csv(url)\n",
        "print(df.head())\n",
        "\n",
        "X = df[['X1', 'X2', 'X3']].values  # Multivariate X (all three features)\n",
        "Y = df['Y'].values\n",
        "\n",
        "# Gradient Descent Function for Multivariate Linear Regression\n",
        "def gradient_descent_multi(X, Y, learning_rate, epochs):\n",
        "    n = len(Y)  # Number of data points\n",
        "    theta = np.zeros(X.shape[1])  # Initialize theta (parameters) to zero for each feature\n",
        "    b = 0  # Initialize intercept to zero\n",
        "    losses = []  # To store loss values\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        # Calculate the prediction\n",
        "        Y_pred = np.dot(X, theta) + b  # Y_pred = theta1*X1 + theta2*X2 + theta3*X3 + b\n",
        "        loss = (1 / (2 * n)) * np.sum((Y_pred - Y) ** 2)\n",
        "        losses.append(loss)\n",
        "\n",
        "        # Gradient calculation\n",
        "        dtheta = -(1/n) * np.dot(X.T, (Y - Y_pred))  # Derivative of loss w.r.t theta\n",
        "        db = -(1/n) * np.sum(Y - Y_pred)  # Derivative of loss w.r.t intercept\n",
        "\n",
        "        # Update parameters\n",
        "        theta -= learning_rate * dtheta\n",
        "        b -= learning_rate * db\n",
        "\n",
        "    return theta, b, losses\n",
        "\n",
        "# Function to plot the results (loss over iterations)\n",
        "def plot_loss(losses, title):\n",
        "    plt.plot(losses)\n",
        "    plt.title(f\"Loss over Iterations with {title}\")\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.show()\n",
        "\n",
        "# Training with all explanatory variables\n",
        "learning_rate = 0.05  # Chosen learning rate\n",
        "epochs = 1000  # Number of iterations\n",
        "\n",
        "# Train the model using gradient descent\n",
        "theta, b, losses = gradient_descent_multi(X, Y, learning_rate, epochs)\n",
        "\n",
        "# Report the final linear model\n",
        "print(f\"Final Model: Y = {theta[0]:.4f}*X1 + {theta[1]:.4f}*X2 + {theta[2]:.4f}*X3 + {b:.4f}\")\n",
        "\n",
        "# Plot loss over iterations\n",
        "plot_loss(losses, f\"Learning Rate {learning_rate}\")\n",
        "\n",
        "# Reporting final loss\n",
        "print(f\"Final loss: {losses[-1]:.4f}\")\n",
        "\n",
        "# Predictions for new data points\n",
        "new_data = np.array([[1, 1, 1], [2, 0, 4], [3, 2, 1]])\n",
        "predictions = np.dot(new_data, theta) + b  # Predicting Y for new inputs\n",
        "for i, pred in enumerate(predictions):\n",
        "    print(f\"Prediction for (X1, X2, X3) = {new_data[i]}: Y = {pred:.4f}\")\n",
        "\n",
        "# Test different learning rates\n",
        "learning_rates = [0.01, 0.05, 0.1]\n",
        "for lr in learning_rates:\n",
        "    theta_lr, b_lr, losses_lr = gradient_descent_multi(X, Y, lr, epochs)\n",
        "    print(f\"\\nLearning rate: {lr}\")\n",
        "    print(f\"Final Model: Y = {theta_lr[0]:.4f}*X1 + {theta_lr[1]:.4f}*X2 + {theta_lr[2]:.4f}*X3 + {b_lr:.4f}\")\n",
        "    print(f\"Final loss: {losses_lr[-1]:.4f}\")\n",
        "    plot_loss(losses_lr, f\"Learning Rate {lr}\")\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "dPggIm7CLJQ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qNtJaB7iRXZ9"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}